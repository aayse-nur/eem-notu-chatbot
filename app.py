import streamlit as st
import os
from dotenv import load_dotenv

from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_community.document_loaders import PyPDFLoader
from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings, ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

# --- 1. Ortam deÄŸiÅŸkenlerini yÃ¼kle ---
load_dotenv()

if "OPENAI_API_KEY" not in os.environ:
    st.error("âŒ LÃ¼tfen `.env` dosyasÄ±na `OPENAI_API_KEY` anahtarÄ±nÄ± ekleyin.")
    st.stop()

# --- 2. RAG pipeline (Ã¶nbelleÄŸe alÄ±nmÄ±ÅŸ) ---
@st.cache_resource
def build_rag_system():
    pdf_path = "data/analog-elektronik.pdf"
    db_path = "chroma_db/faiss_index"

    # Ã–nceden oluÅŸturulmuÅŸ vektÃ¶r veritabanÄ± varsa kullan
    if os.path.exists(db_path):
        vectorstore = FAISS.load_local(db_path, OpenAIEmbeddings(), allow_dangerous_deserialization=True)
        retriever = vectorstore.as_retriever()
    else:
        if not os.path.exists(pdf_path):
            st.error(f"âŒ '{pdf_path}' bulunamadÄ±. LÃ¼tfen PDF dosyasÄ±nÄ± 'data' klasÃ¶rÃ¼ne ekleyin.")
            st.stop()

        # PDF yÃ¼kle
        loader = PyPDFLoader(pdf_path)
        documents = loader.load()

        # Metni parÃ§alara ayÄ±r
        splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)
        chunks = splitter.split_documents(documents)

        # VektÃ¶r veritabanÄ± oluÅŸtur
        embeddings = OpenAIEmbeddings()
        vectorstore = FAISS.from_documents(chunks, embeddings)
        retriever = vectorstore.as_retriever()

        # VektÃ¶r veritabanÄ±nÄ± kaydet
        os.makedirs("chroma_db", exist_ok=True)
        vectorstore.save_local(db_path)

    # Model ve prompt
    llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    prompt = ChatPromptTemplate.from_template("""
AÅŸaÄŸÄ±daki baÄŸlamdan yararlanarak kullanÄ±cÄ± sorusunu yanÄ±tla.
BaÄŸlam yetersizse "Bu bilgi belgeden Ã§Ä±karÄ±lamÄ±yor." de.

KullanÄ±cÄ± ile Ã¶nceki konuÅŸma geÃ§miÅŸi:
{chat_history}

BaÄŸlam:
{context}

Soru:
{question}
""")

    return retriever, llm, prompt

# --- 3. Cevap oluÅŸturma fonksiyonu ---
def create_answer(question, chat_history, retriever, llm, prompt):
    docs = retriever.invoke(question)
    context = "\n\n".join([d.page_content for d in docs])

    chain = prompt | llm | StrOutputParser()

    return chain.invoke({
        "question": question,
        "context": context,
        "chat_history": "\n".join([f"KullanÄ±cÄ±: {q}\nBot: {a}" for q, a in chat_history])
    })

# --- 4. Streamlit arayÃ¼zÃ¼ ---
def main():
    st.set_page_config(page_title="Elektrik ve Elektronik RAG AsistanÄ±", page_icon="âš¡")
    st.title("âš¡ Elektrik ve Elektronik Ders NotlarÄ± AsistanÄ±")
    st.markdown("YÃ¼klenen tÃ¼m Elektrik ve Elektronik ders notlarÄ±nÄ± kullanarak bilgiye dayalÄ± (Grounded RAG) yapay zeka sistemi ile etkileÅŸim kurun. Bu asistan, yalnÄ±zca saÄŸlanan ders materyallerine gÃ¶re cevap Ã¼retir.")

    retriever, llm, prompt = build_rag_system()

    if "chat_history" not in st.session_state:
        st.session_state.chat_history = []

    user_input = st.text_input("Sorunuzu yazÄ±n:", placeholder="Ã–rneÄŸin: Diyot nasÄ±l Ã§alÄ±ÅŸÄ±r?")

    if user_input:
        with st.spinner("YanÄ±t hazÄ±rlanÄ±yor..."):
            answer = create_answer(user_input, st.session_state.chat_history, retriever, llm, prompt)
            st.session_state.chat_history.append((user_input, answer))
            st.markdown(f"**ğŸ¤– Asistan:** {answer}")

if __name__ == "__main__":
    main()
